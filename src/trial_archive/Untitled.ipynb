{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.fa import Persian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = Persian()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "سلام\n",
      "این\n",
      "یک\n",
      "تست\n",
      "است\n",
      ".\n",
      "جمله\n",
      "جالبی\n",
      "،\n",
      "گفته\n",
      "شد\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('سلام این یک تست است. جمله جالبی، گفته شد.')\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "alph = set('ابپتثجچحخدذرزژسشصضطظعغفقکگلمنوهیآاَاِاُ'+' ٚ‌ؤۊئأ') # also has نیم‌فاصله\n",
    "punc = set('.،'+'!؟'+'()')\n",
    "\n",
    "def standard_letter(char):\n",
    "    non_std_to_std = {\n",
    "        'ي': 'ی',\n",
    "        '٫': '،',\n",
    "        ',': '،',\n",
    "        '?': '؟'\n",
    "    }\n",
    "    char = non_std_to_std.get(char, char)\n",
    " \n",
    "    if not (char in alph or char in punc or char in [' ']):\n",
    "        return ''\n",
    "\n",
    "    return char\n",
    "\n",
    "def normalize(text):\n",
    "    # standardize letters\n",
    "    std = ''.join(map(standard_letter, list(text)))\n",
    "    # add whitespace before and after punctuation marks\n",
    "    with_ws = ''.join(map(lambda char: f' {char} ' if char in punc else char, list(text)))\n",
    "    # remove excess whitespace\n",
    "    text = ' '.join(with_ws.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ألمۊت', 'ٚ', 'پيله', 'شر', '،', 'ايران', 'ٚ', 'مئن', 'دۊ', 'ته', 'شر', 'تقسيم', 'بنه', 'یکته', 'خۊراسؤن', 'ٚ', 'ألمۊت', '(', 'شرقي', 'ألٚمۊت', ')', 'کي', 'اين', 'ٚ', 'مرکز', 'بنه', 'معلم\\u200cکلاىه', 'یکته', 'أني', 'أفتؤنيشين', 'ٚ', 'ألمۊت', '(', 'غربي', 'ألمۊت', ')', 'کي', 'اين', 'ٚ', 'مرکز', 'رازمیان', 'ايسسه']\n"
     ]
    }
   ],
   "source": [
    "text = '''\n",
    "ألمۊت ٚ پيله شر، ايران ٚ مئن دۊ ته شر تقسيم بنه یکته خۊراسؤن ٚ ألمۊت (شرقي ألٚمۊت) کي اين ٚ مرکز بنه معلم‌کلاىه یکته أني أفتؤنيشين ٚ ألمۊت (غربي ألمۊت) کي اين ٚ مرکز رازمیان ايسسه. شرقي ألمۊت ىا شرقي رۊدبار ٚألمۊت قزوين ٚ اؤستان ٚ خۊراسؤني کلسيا طرف ؤ تۊنکابۊن ٚ نسا طرف ؤ طالقان ٚ أفتؤنيشين طرف ؤ غربي ألمۊت ٚ خۊراسؤن هننأ. غربي ألمۊت ىا غربي رۊدبار ٚ ألمۊت ني گيلان ؤ مازرۊن ٚ نسا ؤ زىتۊن ٚ رۊبار ٚ شرق ؤ شرقي ألمۊت ٚ غرب ؤ قزوين ٚ اؤستان ٚ کلسيا مئن دره.\n",
    "\n",
    "ألمۊت ٚ نؤمي رۊستاؤن أجي شأنه گازرخان أجي نؤم بردن کي حسن ٚ صباح ٚ ديز اؤره هننأبۊ ىا هير یا وؤشته یا زرآباد یا وربؤن کي خشچال ٚ کۊ هۊن ٚ مئن\n",
    "'''\n",
    "\n",
    "print(word_tokenize(sent_tokenize(normalize(text))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
