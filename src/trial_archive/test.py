from simple_tokenizer import sent_tokenize, word_tokenize
from simple_normalizer import normalize

text = '''
ألمۊت ٚ پيله شر، ايران ٚ مئن دۊ ته شر تقسيم بنه یکته خۊراسؤن ٚ ألمۊت (شرقي ألٚمۊت) کي اين ٚ مرکز بنه معلم‌کلاىه یکته أني أفتؤنيشين ٚ ألمۊت (غربي ألمۊت) کي اين ٚ مرکز رازمیان ايسسه. شرقي ألمۊت ىا شرقي رۊدبار ٚألمۊت قزوين ٚ اؤستان ٚ خۊراسؤني کلسيا طرف ؤ تۊنکابۊن ٚ نسا طرف ؤ طالقان ٚ أفتؤنيشين طرف ؤ غربي ألمۊت ٚ خۊراسؤن هننأ. غربي ألمۊت ىا غربي رۊدبار ٚ ألمۊت ني گيلان ؤ مازرۊن ٚ نسا ؤ زىتۊن ٚ رۊبار ٚ شرق ؤ شرقي ألمۊت ٚ غرب ؤ قزوين ٚ اؤستان ٚ کلسيا مئن دره.

ألمۊت ٚ نؤمي رۊستاؤن أجي شأنه گازرخان أجي نؤم بردن کي حسن ٚ صباح ٚ ديز اؤره هننأبۊ ىا هير یا وؤشته یا زرآباد یا وربؤن کي خشچال ٚ کۊ هۊن ٚ مئن
'''

normalized = normalize(text)
sentences = sent_tokenize(normalized)
for sentence in sentences:
    words = word_tokenize(sentence)
    